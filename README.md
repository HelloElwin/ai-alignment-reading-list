# AI Alignment Reading List

Works for A(G)I can be roughly classified into two categories: making AI more capable, or making it safer and more aligned. While AGI has yet to arrive, its risks are sufficiently high that alignment research with foresight cannot be disregarded or postponed.

As a newcomer to this growing field, I find it hard to navigate through the abundant work, many of which are written as informal posts. Therefore, to help myself and other junior researchers like me, I am maintaining this (personal and subjective) collection of 

- Writings on AI alignment (papers, forum posts, blog posts, etc.)
- Talks, panels, and seminars on AI alignment
- People who are working in this field
- Other related materials

Contributions are greatly welcomed! Please feel free to chat with me, raise an issue, or create a pull request.

## Overview

AI alignment is a young field. Different researchers may have very different research perspectives on what the future path of AI alignment looks like. In the following materials, we may learn about their inspiring perspectives and research agendas, and formulate our own.

I am currently exploring two main topics: [Scalable oversight](#scalable-oversight) and [reward hacking](reward-hacking). Many other interesting topics are covered in:

- ***The Alignment Workshop** (2023)* [[talks]](https://www.alignment-workshop.com) Researchers from top industry AI labs (OpenAI, DeepMind, Anthropic) and universities (Cambridge, NYU) co-organized a two-day invite-only alignment workshop. It provides a window into how leading researchers are grappling with one of the most pressing problems of our time.
- ***Current Work in AI Alignment** (2019)* [[talk]](https://www.youtube.com/watch?v=-vsYtevJ2bc) [[post&discussions]](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment) Paul Christiano's framing of his research agenda in 2019.
- ***Concrete Problems in AI Safety** (2016)* [[paper]](https://arxiv.org/abs/1606.06565)
- ***Unsolved Problems in ML Safety** (2022)* [[paper]](https://arxiv.org/abs/2109.13916)

## Scalable oversight

- ***Measuring Progress on Scalable Oversight for Large Language Models** (2022)* [[paper]](https://arxiv.org/abs/2211.03540) Human supervision is limited, then how can we scalably align models that are more capable than us? This work lays out a research agenda based on Cotra's sandwiching proposal and performs a relaxed experiment with LLMs.
- ***Constitutional AI: Harmlessness from AI Feedback** (2022)* [[paper]](https://arxiv.org/abs/2212.08073) Training a harmless AI assistant through self-improvement based on a list of human-designed rules.

## Reward hacking

- ***The Alignment Problem from a Deep Learning Perspective** (2023)* [[paper]](https://arxiv.org/abs/2209.00626) Assume that AGI is trained using RL-like methods, how can we due with resulting issues like situationally-aware reward hacking, misaligned internally-represented goals, and power-seeking during deployment? This work explains these challenges and introduces possible solutions.
- ***Do the Reward Justify the Means? Measuring Trade-Offs Between...** (2023)* [[paper]](https://arxiv.org/abs/2304.03279) Evaluate how LLMs trade off rewards and ethics in a set of Choose-Your-Own-Adventure games and tried some improvements. Harmful and power-seeking behaviors etc. are mathematically formalized.

## People

An incomplete list of leading researchers in AI alignment. Still in construction.

### University

- [Jacob Steinhardt](https://jsteinhardt.stat.berkeley.edu)
- [Sam Bowman](https://cims.nyu.edu/~sbowman/)
- [David Krueger](https://www.davidscottkrueger.com)

### Industry

- [Jan Leike](https://jan.leike.name)
- [Richard Ngo](https://www.richardcngo.com)
- [Paul Christiano](https://paulfchristiano.com)
