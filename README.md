# AI Alignment Reading List

Works for A(G)I can be roughly classified into two categories: making AI more capable, or making it safer and more aligned. While AGI has yet to arrive, its risks are sufficiently high that alignment research with foresight cannot be disregarded or postponed.

As a newcomer to this growing field, I find it hard to navigate through the abundant work, many of which are written as informal posts. Therefore, to help myself and other junior researchers like me, I am maintaining this collection of 

- Writings on AI alignment (papers, forum posts, blog posts, etc.)
- Talks, panels, and seminars on AI alignment
- People who are working in this field
- Other related materials

Contributions are greatly welcomed! Please feel free to chat with me, raise an issue, or create a pull request.

## Overview

AI alignment is a young field. Different researchers may have very different research perspectives on what the future path of AI alignment looks like. In the following materials, we may learn about their inspiring perspectives and research agendas, and formulate our own.

- ***The Alignment Problem from a Deep Learning Perspective** (2023)* [[paper]](https://arxiv.org/abs/2209.00626) Assume that AGI is trained using RL-like methods, how can we due with resulting issues like situationally-aware reward hacking, misaligned internally-represented goals, and power-seeking during deployment? This work explains these challenges and introduces possible solutions.
- ***Measuring Progress on Scalable Oversight for Large Language Models** (2022)* [[paper]](https://arxiv.org/abs/2211.03540) Human supervision is limited, then how can we scalably align models that are more capable than us? This work lays out a research agenda based on Cotra's sandwiching proposal and performs a relaxed experiment with LLMs.
- ***Current Work in AI Alignment** (2019)* [[talk]](https://www.youtube.com/watch?v=-vsYtevJ2bc) [[post&discussions]](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment) Paul Christiano's framing of his research agenda in 2019.
- ***The Alignment Workshop** (2023)* [[talks]](https://www.alignment-workshop.com) Researchers from top industry AI labs (OpenAI, DeepMind, Anthropic) and universities (Cambridge, NYU) co-organized a two-day invite-only alignment workshop. It provides a window into how leading researchers are grappling with one of the most pressing problems of our time.
- ***Concrete Problems in AI Safety** (2016)* [[paper]](https://arxiv.org/abs/1606.06565)
- ***Unsolved Problems in ML Safety** (2022)* [[paper]](https://arxiv.org/abs/2109.13916)

## People

An incomplete list of leading researchers in AI alignment. Still in construction.

### University

- [Jacob Steinhardt](https://jsteinhardt.stat.berkeley.edu)
- [Sam Bowman](https://cims.nyu.edu/~sbowman/)
- [David Krueger](https://www.davidscottkrueger.com)

### Industry

- [Jan Leike](https://jan.leike.name)
- [Richard Ngo](https://www.richardcngo.com)
- [Paul Christiano](https://paulfchristiano.com)
