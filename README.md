# AI Alignment Reading List

Works for A(G)I can be roughly classified into two categories: making AI more capable, or making it more aligned. While AGI has yet to arrive, its risks are sufficiently high that alignment research with foresight cannot be disregarded or postponed.

As a newcomer to this growing field, I find it hard to navigate through the abundant work, many of which are written as informal posts. Therefore, to help myself and other junior researchers like me, I am maintaining this collection of 

- Writings on AI alignment (papers, forum posts, blog posts, etc.)
- Talks, panels, and seminars on AI alignment
- People who are working in this field
- Other related materials

Contributions are greatly welcomed! Please feel free to chat with me, raise an issue, or create a pull request.

## Overview

AI alignment is a young field. Different researchers may have very different researchers perspectives on what the future path of AI alignment looks like. In the following materials, we may learn about their inspiring perspectives and research agendas, and formulate our own.

- ***The Alignment Problem from a Deep Learning Perspective** (2023)* [[paper]](https://arxiv.org/abs/2209.00626) 
- ***Measuring Progress on Scalable Oversight for Large Language Models** (2022)* [[paper]](https://arxiv.org/abs/2211.03540)
- ***Current work in AI alignment** (2019)* [[talk]](https://www.youtube.com/watch?v=-vsYtevJ2bc) [[post&discussions]](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment)
- ***Concrete Problems in AI Safety** (2016)* [[paper]](https://arxiv.org/abs/1606.06565)
- ***Unsolved Problems in ML Safety** (2022)* [[paper]](https://arxiv.org/abs/2109.13916)

## People

todo
